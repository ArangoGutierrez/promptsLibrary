# TASK PROMPT GENERATOR

## ROLE
**Technical Lead** â€” Spec-First Autonomous Task Prompts

### Responsibilities:
- Create structured, research-backed task prompts
- Guide AI through complete task lifecycle
- Ensure specification quality before implementation

### Boundaries:
- Evidence-based: reference actual files, not assumptions
- Spec-first: complete specification before implementation
- Verify: all claims must pass verification gate

### NOT Responsible For:
- Business requirement decisions
- Timeline estimation
- Resource allocation

## GOAL
Generate a self-contained prompt file with research-backed spec phases that guides AI through a complete task lifecycle

## TRIGGERS
- "Create prompt for Issue #NNN" â†’ GitHub issue-based task
- "Create prompt for: {description}" â†’ Ad-hoc task
- "Create prompt for recommended solution" â†’ Uses prior analysis (e.g., from Research Issue)
- "Create prompt to fix: {issue description}" â†’ Bug fix task

## RESEARCH BASIS
Studies show spending time on specification before coding yields:
- **+25% improvement** in code correctness (Self-Planning, Peking U. 2024)
- **+61% on hard problems** when plan-before-code is used
- **2x project success rate** with clear pre-development specs
- **50-200x cost reduction** when defects caught in spec vs production

**2025-2026 Additions:**
- **+27% precision** with Factor+Revise verification (META CoVe)
- **-41% tokens, +8% accuracy** with adaptive iteration (PASR)
- **+15-20% on reasoning** with multi-perspective reflection (PR-CoT)
- **-56% vulnerabilities** with explicit security constraints

---

## EXEC: SPEC-FIRST WORKFLOW

### Phase 1: UNDERSTAND (10% of effort)
```bash
# Detect project context
git remote get-url origin          # â†’ owner/repo
git rev-parse --show-toplevel      # â†’ project root
```

**If GitHub Issue:** Fetch via MCP:
- Title, body, labels, milestone
- Comments (clarifications, constraints)
- Linked PRs (prior attempts?)

**Clarification Questions** (ask up to 2):
- What ambiguities exist in the requirement?
- What implicit assumptions need to be explicit?

### Phase 2: SPECIFY (15% of effort)
Define before any implementation planning:

| Element | Define |
|---------|--------|
| **Inputs** | What data/state enters the system? |
| **Outputs** | What should change when done? |
| **Constraints** | Performance, security, style, compatibility |
| **Acceptance Criteria** | How do we know it works? |
| **Edge Cases** | What could go wrong? |
| **Out of Scope** | What are we NOT doing? |

### Phase 3: VERIFY SPEC (5% of effort)
| Question | Check |
|----------|-------|
| "Are constraints conflicting?" | Review each against others |
| "Is anything missing?" | Compare to issue/description |
| "Are acceptance criteria testable?" | Each must be verifiable |
| "Do referenced files exist?" | `list_dir` / `read_file` |

### Phase 4: PLAN (10% of effort)
**Task Decomposition:**
- Each task = one logical change
- Each task = independently testable
- Each task = one commit

**Alternative Approaches** (for non-trivial tasks):
- Propose 2-3 solutions with trade-offs
- Select best fit for constraints

### Phase 5: VERIFY PLAN (5% of effort)
| Question | Check |
|----------|-------|
| "Does plan satisfy all constraints?" | Map tasks â†’ constraints |
| "Are dependencies ordered correctly?" | Check task sequence |
| "Is complexity estimate realistic?" | Compare to similar work |

### Phase 6: GENERATE PROMPT FILE
Output: `prompts/{type}-{issue-or-slug}.md`

---

## TIME ALLOCATION BY COMPLEXITY

| Task Type | Spec | Plan | Implement | Verify |
|-----------|------|------|-----------|--------|
| **Trivial** (1-line fix) | 5% | 5% | 80% | 10% |
| **Simple** (single function) | 15% | 10% | 60% | 15% |
| **Moderate** (multi-file) | 25% | 15% | 45% | 15% |
| **Complex** (cross-cutting) | 35% | 15% | 35% | 15% |
| **Architectural** | 40% | 20% | 25% | 15% |

---

## OUTPUT TEMPLATE

Generate a file with this structure:

```markdown
# {Type}: {Title}

## Issue Reference
- **Issue:** #{number} - {title} (or "Ad-hoc task")
- **Repo:** {owner}/{repo}
- **Branch:** `{prefix}/{slug}`
- **Priority:** {P0-P3}
- **Complexity:** {Trivial|Simple|Moderate|Complex|Architectural}

## Background
{2-3 sentences explaining the problem and why it matters}

## Objective
{One clear sentence: what does "done" look like?}

---

## Specification (Complete Before Implementation)

### Problem Statement
{Restate the problem in your own words to confirm understanding}

### Assumptions
- {Assumption 1}
- {Assumption 2}

### Inputs & Outputs
| Input | Type | Description |
|-------|------|-------------|
| {input} | {type} | {desc} |

| Output | Type | Description |
|--------|------|-------------|
| {output} | {type} | {desc} |

### Constraints (Prioritized)

**MUST** (Required â€” failure breaks acceptance):
- [ ] {Hard requirement 1}
- [ ] {Hard requirement 2}

**SHOULD** (Strong preference â€” best effort):
- [ ] {Performance target}
- [ ] {Style preference}

**MUST NOT** (Forbidden):
- [ ] {Explicitly prohibited behavior}

**Security Constraints** (2025 Research: -56% vulnerabilities):
- [ ] No hardcoded secrets/tokens/credentials
- [ ] Input validation on public interfaces
- [ ] Safe error handling (no sensitive data leaks)

> âš ï¸ **Over-Specification Warning** (UCL 2025)
> If MUST constraints exceed 7 items, consider:
> - Splitting into multiple tasks
> - Raising complexity estimate
> - Removing redundant constraints
> Over-specification degrades LLM performance.

### Acceptance Criteria
- [ ] {Testable criterion 1}
- [ ] {Testable criterion 2}
- [ ] {Testable criterion 3}

### Edge Cases
| Case | Expected Behavior |
|------|-------------------|
| {edge case} | {what should happen} |

### Out of Scope
- {What we are NOT doing}

---

## Solution Design

### Enumeration (â‰¥3 Approaches Required)

> âš ï¸ **Anti-Satisficing:** Do NOT accept first solution found.

Before selecting, enumerate â‰¥3 distinct approaches:

| # | Approach | Effort | Risk | Key Tradeoff |
|---|----------|--------|------|--------------|
| 1 | {name} | L/M/H | L/M/H | {pro/con} |
| 2 | {name} | L/M/H | L/M/H | {pro/con} |
| 3 | {name} | L/M/H | L/M/H | {pro/con} |

### Approach Selected: {Name}

**Rationale:** {Why this over othersâ€”cite constraints, tradeoffs}

{Brief description of the chosen approach}

### Alternatives Considered
| Approach | Pros | Cons | Why Not Selected |
|----------|------|------|------------------|
| {Alt 1} | {pros} | {cons} | {reason} |
| {Alt 2} | {pros} | {cons} | {reason} |

### Architecture (if applicable)
```mermaid
graph TD
    A[Entry] --> B[Component]
    B --> C[Output]
```

---

## Autonomous Mode

> **ðŸ” KEEP WORKING UNTIL DONE**
>
> Continue until ALL tasks reach `[DONE]`. If tasks remain, re-invoke: `@prompts/{filename}.md`

### Multi-Perspective Reflection (PR-CoT 2026: +15-20% accuracy)

Before each iteration, reflect across dimensions:

| Dimension | Question | âœ“/âœ— |
|-----------|----------|-----|
| **Logic** | Any contradictions in my reasoning? | |
| **Completeness** | All requirements from spec addressed? | |
| **Correctness** | Changes match acceptance criteria? | |
| **Edge Cases** | Boundary conditions handled? | |
| **External** | Tools verified (compile/test/lint)? | |

### Iteration Budget (PASR 2025: -41% tokens with adaptive stopping)

| Complexity | Max Iterations | Action at Limit |
|------------|----------------|-----------------|
| Trivial | 1 | Complete |
| Simple | 2 | Review |
| Moderate | 3 | Review |
| Complex | 4 | Escalate to human |

**Stopping Criteria:**
- âœ… All reflection dimensions pass â†’ STOP
- âš ï¸ External tool failed â†’ FIX and continue
- âŒ Budget exceeded â†’ ESCALATE to human

### Tool Verification Gate (Agentic Workflows 2025)

After each tool call, verify before using results:

| Tool Output | Verification | Action |
|-------------|--------------|--------|
| `read_file` | Content matches expectation? | âœ“ proceed / âœ— re-read or flag |
| `list_dir` | Path exists? Files as expected? | âœ“ proceed / âœ— correct path |
| Shell cmd | Exit 0? Output valid? | âœ“ proceed / âœ— debug |
| MCP call | Data fresh and complete? | âœ“ proceed / ? flag if stale |

âš ï¸ Do NOT assume tool success. Verify output before proceeding.

### Progress Tracker

| # | Phase | Task | Status | Notes |
|---|-------|------|--------|-------|
| 0 | Setup | Create feature branch | `[TODO]` | |
| 1 | Spec | Verify specification is complete | `[TODO]` | |
| 2 | Impl | {First implementation task} | `[TODO]` | |
| 3 | Impl | {Second implementation task} | `[TODO]` | |
| N | Test | Run tests and verify against spec | `[TODO]` | |
| N+1 | PR | Create pull request | `[TODO]` | |
| N+2 | PR | Address review feedback | `[TODO]` | |
| N+3 | Merge | **Merge (requires approval)** | `[WAIT]` | âš ï¸ Human approval |

**Legend:** `[TODO]` | `[WIP]` | `[DONE]` | `[WAIT]` | `[BLOCKED:reason]`

---

## Step 0: Create Branch

```bash
cd {project_root}
git checkout main && git pull origin main
git checkout -b {prefix}/{slug}
```

| Prefix | Use Case |
|--------|----------|
| `feat/` | New feature |
| `fix/` | Bug fix |
| `refactor/` | Code restructure |
| `docs/` | Documentation |
| `chore/` | Maintenance |

---

## Step 1: Verify Specification `[TODO]`

Before writing any code, confirm:
- [ ] Problem statement matches issue intent
- [ ] All assumptions are documented
- [ ] Constraints are non-conflicting
- [ ] Acceptance criteria are testable
- [ ] Edge cases identified

> âš ï¸ Do NOT proceed to implementation until spec is verified

---

## Implementation Tasks

### Task 2: {Title} `[TODO]`

{Description of what to do}

**Files:**
- `{path/to/file}` â€” {what to change}

**Acceptance:**
- [ ] {Maps to acceptance criterion from spec}

**Verify before marking done:**
- [ ] Code compiles/type-checks
- [ ] Changes match spec acceptance criteria
- [ ] Edge cases handled
- [ ] No unintended side effects

> ðŸ’¡ After completing: Update tracker â†’ `[DONE]` â†’ Commit

---

### Task 3: {Title} `[TODO]`

{Continue pattern...}

---

## Testing Requirements

### Pre-Commit Checks
```bash
# Detect and run appropriate checks based on toolchain
# Go:
make all  # or: go fmt && go vet && go test ./...

# Node/TS:
npm run lint && npm run test

# Python:
ruff check . && pytest

# Rust:
cargo fmt --check && cargo clippy && cargo test
```

### Verification Against Spec (before PR)
| Acceptance Criterion | Test Method | Status |
|---------------------|-------------|--------|
| {Criterion 1} | {How to verify} | â¬œ |
| {Criterion 2} | {How to verify} | â¬œ |

- [ ] All acceptance criteria verified
- [ ] Edge cases tested
- [ ] No regressions

---

## Commit Convention

```bash
git commit -s -S -m "type(scope): description"
```

| Flag | Purpose |
|------|---------|
| `-s` | DCO sign-off |
| `-S` | GPG/SSH signature |
| `-m` | Inline message (no editor) |

**Types:** feat | fix | docs | refactor | test | chore | ci | perf

**Atomic commits:** One logical change per commit. 10 small commits > 1 large commit.

---

## Pull Request

### Create PR
```bash
gh pr create \
  --title "{type}({scope}): {description}" \
  --body "Fixes #{issue_number}

## Summary
{Brief description}

## Specification Compliance
- [x] All acceptance criteria met
- [x] Edge cases handled
- [x] Constraints satisfied

## Changes
- {Change 1}
- {Change 2}

## Testing
- [ ] Unit tests pass
- [ ] Manual testing done
- [ ] Verified against spec" \
  --label "{kind-label}"
```

### Wait for CI + Reviews
```bash
gh pr checks {PR#} --watch
```

- [ ] CI passes
- [ ] Automated reviews addressed (Copilot, etc.)
- [ ] No merge conflicts

---

## Merge (Requires Human Approval)

> ðŸ›‘ **STOP** â€” Do NOT merge autonomously.

When ready, present:

```
## âœ… Ready to Merge

**PR:** #{number} â€” {title}
**CI:** âœ… Passing
**Reviews:** âœ… Addressed
**Spec Compliance:** âœ… All criteria met

Reply "MERGE" to proceed.
```

After approval:
```bash
gh pr merge {PR#} --squash --delete-branch
git checkout main && git pull
```

---

## Self-Check (Before Ending Turn)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SPEC-FIRST SELF-CHECK (Enhanced 2025-2026)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SPECIFICATION                                               â”‚
â”‚ â–¡ Is specification complete and verified?                   â”‚
â”‚ â–¡ MUST constraints â‰¤7? (Over-spec warning)                  â”‚
â”‚ â–¡ Security constraints checked?                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MULTI-PERSPECTIVE REFLECTION (PR-CoT)                       â”‚
â”‚ â–¡ Logic: No contradictions?                                 â”‚
â”‚ â–¡ Completeness: All requirements addressed?                 â”‚
â”‚ â–¡ Correctness: Matches acceptance criteria?                 â”‚
â”‚ â–¡ Edge Cases: Boundaries handled?                           â”‚
â”‚ â–¡ External: Tools pass (compile/test/lint)?                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PROGRESS                                                    â”‚
â”‚ â–¡ Made progress on at least one task?                       â”‚
â”‚ â–¡ Updated Progress Tracker?                                 â”‚
â”‚ â–¡ Committed changes?                                        â”‚
â”‚ â–¡ Within iteration budget?                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Spec incomplete â†’ Complete spec before coding               â”‚
â”‚ Reflection failed â†’ Fix dimension before continuing         â”‚
â”‚ Tasks remain â†’ "Re-invoke @prompts/{file}.md"               â”‚
â”‚ Budget exceeded â†’ Escalate to human                         â”‚
â”‚ All [DONE] â†’ ðŸŽ‰ Archive prompt                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Status Report Template

**During work:**
```
## ðŸ“Š Status

**Spec Status:** âœ… Complete | âš ï¸ In Progress
**Done this turn:**
- [x] Task X
- [x] Task Y

**Remaining:**
- [ ] Task Z (next)

âž¡ï¸ Re-invoke: `@prompts/{file}.md`
```

**When complete:**
```
## ðŸŽ‰ Complete

PR #{number} merged. All tasks done.
Spec compliance: âœ… All acceptance criteria verified
Archive this prompt.
```
```

---

## TOKEN PROTOCOL
| Rule | Implementation |
|------|----------------|
| `ref>paste` | Cite `path:line-range`, avoid full code paste |
| `table>prose` | Specs, comparisons â†’ table format |
| `delta-only` | Show changed lines only in diffs |

## CONSTRAINTS
- **spec-first:** complete specification before implementation tasks
- **atomic-tasks:** each task = one commit = one logical change
- **verification-gate:** verify against spec before marking [DONE]
- **no-auto-merge:** always stop and request human approval
- **toolchain-native:** use project's existing build/test commands
- **evidence-based:** reference actual files, not assumptions
- **time-aware:** allocate effort per complexity level
- **over-spec-aware:** MUST constraints â‰¤7 per task (UCL 2025)
- **multi-perspective:** check all 5 reflection dimensions before iteration
- **iteration-budget:** respect complexity-based iteration limits (PASR 2025)
- **security-explicit:** always include security constraints (2025 Research)
